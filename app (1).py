"""
ğŸ¤– RAG TabanlÄ± CV Chatbot - OpenAI/Google Gemini Versiyonu
Bu uygulama Nil YaÄŸmur Muslu'nun CV bilgilerini kullanarak sorulara cevap verir.
API Key artÄ±k gÃ¼venli bir ÅŸekilde .env veya Hugging Face Secrets Ã¼zerinden alÄ±nÄ±r.
"""

# ===============================
# BÃ–LÃœM 1: Gerekli KÃ¼tÃ¼phaneler
# ===============================
import os
from dotenv import load_dotenv  # .env dosyasÄ±ndan API key yÃ¼klemek iÃ§in
import warnings
warnings.filterwarnings('ignore')  # gereksiz uyarÄ±larÄ± gizle

import streamlit as st  # web arayÃ¼zÃ¼
from langchain.text_splitter import RecursiveCharacterTextSplitter  # metni parÃ§alara bÃ¶lmek iÃ§in
from langchain_community.vectorstores import FAISS  # vektÃ¶r veri tabanÄ±
try:
    from langchain_community.embeddings import HuggingFaceEmbeddings  # embedding modeli
except ImportError:
    from langchain.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI  # Google Gemini LLM
import pickle  # veri tabanÄ± kaydetmek ve yÃ¼klemek iÃ§in

# ===============================
# BÃ–LÃœM 2: API Key YÃ¼kleme
# ===============================
# .env dosyasÄ±nÄ± yÃ¼kle (lokalde)
load_dotenv()

# Hugging Face Secrets veya .env Ã¼zerinden API key al
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# ===============================
# BÃ–LÃœM 3: Sabitler
# ===============================
DATA_FILE = "data.txt"  # CV verisi
FAISS_INDEX_PATH = "./faiss_index.pkl"  # FAISS index dosyasÄ±
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# ===============================
# BÃ–LÃœM 4: Fonksiyonlar
# ===============================
@st.cache_resource  # tekrar tekrar hesaplamamak iÃ§in
def load_vector_store():
    """
    FAISS vektÃ¶r veritabanÄ±nÄ± yÃ¼kler veya oluÅŸturur.
    EÄŸer diskte mevcutsa yÃ¼kler, yoksa data.txt'den oluÅŸturur.
    """
    if os.path.exists(FAISS_INDEX_PATH):
        with open(FAISS_INDEX_PATH, 'rb') as f:
            vectorstore = pickle.load(f)
        return vectorstore
    else:
        with open(DATA_FILE, 'r', encoding='utf-8') as file:
            text = file.read()

        # Metni chunk'lara ayÄ±r
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100,
            separators=["\n### ", "\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_text(text)

        # Embeddings oluÅŸtur
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # FAISS index oluÅŸtur
        vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)

        # Kaydet
        with open(FAISS_INDEX_PATH, 'wb') as f:
            pickle.dump(vectorstore, f)

        return vectorstore

def get_response(question: str, vectorstore):
    """
    KullanÄ±cÄ± sorusuna RAG + Google Gemini kullanarak cevap Ã¼retir.
    """
    try:
        # LLM yapÄ±landÄ±rmasÄ±
        llm = ChatGoogleGenerativeAI(
            model="gemini-pro",
            temperature=0.3,
            google_api_key=GOOGLE_API_KEY  # gizli anahtar kullanÄ±lÄ±yor
        )

        # En alakalÄ± chunk'larÄ± bul
        retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
        relevant_docs = retriever.invoke(question)

        # Context oluÅŸtur
        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        # Prompt oluÅŸtur
        prompt = f"""Sen Nil YaÄŸmur Muslu'nun kiÅŸisel CV asistanÄ±sÄ±n.
BAÄLAM:
{context}
SORU: {question}
YANITLAMA KURALLARI:
1. Sadece baÄŸlamÄ± kullan
2. TÃ¼rkÃ§e yanÄ±tla
3. Samimi ve doÄŸal ol
4. Bilgi yoksa "Bu konuda bilgim yok" de
5. KÄ±sa ve Ã¶z ol
YANIT:"""

        # Cevap Ã¼ret
        response = llm.invoke(prompt)
        return response.content

    except Exception as e:
        return f"âŒ Hata: {str(e)}"

# ===============================
# BÃ–LÃœM 5: Web ArayÃ¼zÃ¼
# ===============================
def main():
    st.set_page_config(
        page_title="Nil YaÄŸmur Muslu - CV Chatbot",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("ğŸ¤– Nil YaÄŸmur Muslu - CV Chatbot")
    st.markdown("### KiÅŸisel Asistan")
    st.info("ğŸ’¬ **Merhaba!** Ben Nil YaÄŸmur Muslu'nun CV asistanÄ±yÄ±m. SorularÄ±nÄ± sorabilirsin.")

    # Vector store yÃ¼kle
    try:
        with st.spinner("â³ Vector store yÃ¼kleniyor..."):
            vectorstore = load_vector_store()
        st.success("âœ… Sistem hazÄ±r!")
    except Exception as e:
        st.error(f"âŒ Hata: {e}")
        return

    # Chat geÃ§miÅŸi
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Chat geÃ§miÅŸini gÃ¶ster
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ± giriÅŸi
    if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Asistan cevabÄ±
        with st.chat_message("assistant"):
            with st.spinner("ğŸ’­ DÃ¼ÅŸÃ¼nÃ¼yorum..."):
                response = get_response(prompt, vectorstore)
            st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})

    # Footer
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center'><p>ğŸ’» Nil YaÄŸmur Muslu'nun kiÅŸisel CV asistanÄ±</p></div>",
        unsafe_allow_html=True
    )

# EÄŸer doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rsa
if __name__ == "__main__":
    main()

